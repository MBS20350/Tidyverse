---
title: "Tidyverse Modeling Project"
author:  Mark Sucato
output: 
  html_document:
    keep_md: true 
---

```{r include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning=FALSE, fig.align="center", message=FALSE, error=FALSE)
```
<center>

# Tidyverse Modeling Project  
### by Mark Sucato

</center>

## Summary

Utilization of a tuned Random Forest machine learning algorithm, trained on a 
data set composed of 90,975 observations and heavily reliant on keyword-based 
text analysis of free-text complaint information, produced a final accuracy of
 96.6% against the training dataset and a predicted out-of-sample error rate
 of 14.59%.  The model excelled at predicting card-based accounts, but struggled
 with auto and student loans.  This weakness is almost certainly due to product 
distrubutions within the training set.  Any test data with large amounts of non-mortgage 
loans will likely see higher error rates than predicted here.   


## Background

The Consumer Financial Protection Bureau (CFPB) is an independent agency of the United States
 government that promotes transparency and protects consumers by providing information needed to
 make decisions when choosing financial institutions, including banking institutions, lenders,
 mortgage services,credit unions, securities firms, foreclosure services, and debt collectors.
 The CFPB receives and processes complaints and questions about consumer financial products and
 services.  In support of these tasks, the CFPB maintains the Consumer Complaint Database, which
records a variety of attributes about each complaint, including free text input of complaint
 specifics.  

## Training Data

The *Modeling Data in the Tidyverse*-provided training dataset includes 90,975 observations of
six variables:  
* product 
* customer narrative
* company
* state
* zip code
* submission method  

Initial exploratory analysis indicates no missing or blank observations.  Additionally, all 
complaints were submitted via the internet(web).

```{r}
library(tidyverse)
library(tidymodels)
library(skimr)
library(tidytext)
library(corpus)
library(tm)
library(corrplot)
library(randomForest)
library(vip)
library(knitr)
set.seed(1234)

train <- read_csv("data_complaints_train.csv")
names(train) <- make.names(names(train), unique = TRUE) 
skim(train) 
```
 
## Initial Data Wrangling Approach

This project utilizes text analysis to identify the 15 most commonly used words for each of the
four product types in the training dataset.  These *keywords* are then combined and used to 
build a DocumentTermMatrix (DTM) that, once combined with the other four predictor variables, forms the
initial Machine Learning feature list.  Specific steps include:  
* Parse the Consumer Complaint Narratives to remove "xx" sanitized details, stop words and
unnecessary spaces.  Results are then reduced to stem words to aid analysis(*parsedPreProc*).
Because every observation comes from the internet, *submitted.via* is also removed as a potential
predictor. 
* For each of the four provided product types, identify the 15 most commonly used keywords
 (*parsedCard, parsedVeh, parsedMortg & parsedStu*)
* Aggregate the keywords into one list *keywords*, which is then used to build a training
dataset DTM of 28 unique keywords (*trainDTM*). 
* Transform *trainDTM* into a tidy dataset and combine it with the predictor variables provided
in the orginal dataset (*Train*).    

``` {r}
train <- train %>%
	select(-Submitted.via) %>%
	mutate(across(c(Product, Company, State, ZIP.code), as.factor)) %>%
	mutate(temp_id = c(1:length(Product)))
parsedPreProc <- train %>%
  select(temp_id, Product, Consumer.complaint.narrative) %>%
  unnest_tokens(word, Consumer.complaint.narrative) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "xx")) %>%
  filter(!str_detect(word, "\\d")) %>%
  filter(!str_detect(word, "\\s")) %>%
  mutate(word = text_tokens(word, stemmer = "en")) 

parsedCard <- parsedPreProc %>%
  filter(Product == "Credit card or prepaid card") %>%
  select(-Product) %>%
  count(temp_id, word) %>%
  group_by(word) %>%
  summarise (total = sum(n)) %>%
  arrange(desc(total)) %>%
  slice(1:15)
parsedVeh <- parsedPreProc %>%
  filter(Product == "Vehicle loan or lease") %>%
  select(-Product) %>%
  count(temp_id, word) %>%
  group_by(word) %>%
  summarise (total = sum(n)) %>%
  arrange(desc(total)) %>%
  slice(1:15)
parsedMortg <- parsedPreProc %>%
  filter(Product == "Mortgage") %>%
  select(-Product) %>%
  count(temp_id, word) %>%
  group_by(word) %>%
  summarise (total = sum(n)) %>%
  arrange(desc(total)) %>%
  slice(1:15)
parsedStu <- parsedPreProc %>%
  filter(Product == "Student loan") %>%
  select(-Product) %>%
  count(temp_id, word) %>%
  group_by(word) %>%
  summarise (total = sum(n)) %>%
  arrange(desc(total)) %>%
  slice(1:15)

keywords1 <- full_join(parsedCard, parsedMortg)
keywords2 <- full_join(parsedStu, parsedVeh)
keywords <- full_join(keywords1, keywords2) %>%
  select(word) %>%
  distinct(word)
parsedTrain <- train %>%
	select(temp_id, Consumer.complaint.narrative) %>%
	unnest_tokens(word, Consumer.complaint.narrative) %>%
	mutate(word = text_tokens(word, stemmer = "en")) %>%
  semi_join(keywords) %>%
	count(temp_id, word)  
trainDTM <- parsedTrain %>%
	cast_dtm(temp_id, word, n)

tidyTrain <- trainDTM %>%
	tidy() %>%
	rename(temp_id = document) %>%
	pivot_wider(names_from = term, 
			values_from = count, 
			names_repair = "minimal", 
			values_fill = 0) %>%
	mutate(temp_id = as.numeric(temp_id)) %>%
	arrange(temp_id) %>%
	left_join(train, by = "temp_id") %>%
	select(-Consumer.complaint.narrative)
Train <- tidyTrain %>%
	select(-temp_id)
```

A correlation plot of the remaining feature variables confirms relatively little 
correlation between parsed-text features.

```{r, fig.width=12}
trainCor <- cor(Train %>% select_if(is.numeric))
corrplot(trainCor, tl.cex = 0.5)
```

## Initial modeling

A Random Forest approach within the tidymodel ecosystem is used to initially
 model the data.  The initial recipe uses the combined dataset, replaces
all factor features with dummy numerical variables and checks for variables
with zero variance. Analysis of the predictor variables' importance indicates
 ZIP codes and states are not leading predictors within the training set.

```{r, fig.width=12}
allRecipe <- Train %>%
	recipe(Product ~ .) %>%
	step_dummy(Company, State, ZIP.code, one_hot = TRUE) %>%
	step_nzv(all_predictors())
RFmodel <- rand_forest(mtry=10, min_n = 4) %>%
  set_engine("randomForest") %>%
  set_mode("classification")
RFwork <- workflow() %>%
  add_recipe(allRecipe) %>%
  add_model(RFmodel)
RFfit <- fit(RFwork, data = Train)

RFresults <- RFfit %>%
  pull_workflow_fit() %>%
  vip(num_features = 30)
RFresults
```

Without any cross-validation, this model predicted the training dataset with
97.3% accuracy.  The Out-of-Bag estimate of error is 14.45%, implying an out
of sample accuracy of 85.55%.  Examination of the confusion matrix indicates
the model accurately predicted card accounts, but struggled with non-mortgage
 loans. 

```{r, results="hold"}
RFTrngPred <- predict(RFfit, new_data = Train)
RFTrngAcc <- accuracy(Train, truth = Product, estimate = RFTrngPred$.pred_class) 
RFTrngAcc
RFfit 
```

## Revised model

Initially, the same model is tested against a training dataset with state and
zip code predictors removed in the interest of reduced computation time. This 
produced marginally less accuracy against the training set in exchange for slight
improvements in computation time.

```{r, fig.width=12, results="hold"} 
Train2 <- Train %>%
  select(-State, -ZIP.code)
modRecipe <- Train2 %>%
  recipe(Product ~ .) %>%
  step_dummy(Company, one_hot = TRUE) %>%
  step_nzv(all_predictors())
RFwork2 <- workflow() %>%
  add_recipe(modRecipe) %>%
  add_model(RFmodel)
RFfit2 <- fit(RFwork2, data = Train2)

RFresults2 <- RFfit2 %>%
  pull_workflow_fit() %>%
  vip(num_features = 30)
RFresults2

RFTrngPred2 <- predict(RFfit2, new_data = Train2)
RFTrngAcc2 <- accuracy(Train2, truth = Product, estimate = RFTrngPred2$.pred_class) 
RFTrngAcc2
```

## Modeling with Cross-Validation & Tuning

To regain the lost accuracy, cross-validation and tuning is introduced. The
tuned "best" values are set into the model to produce the final result.  The
final "tuned" model did not regain the marginal accuracy against the training set, 
with an accuracy of 96.6% against the training set and an estimated error of
85.41% against out-of-sample data.  As before, the final model excels with 
card-based accounts but struggles categorizing student and auto loans.  This 
almost certainly traces to the card-heavy distribution of products within the 
training set.

```{r, results="hold"}
vfRF <- vfold_cv(data = Train2, v = 10)
vfRFmodel <- rand_forest(mtry = tune(), min_n = 4) %>%
  set_engine("randomForest") %>%
  set_mode("classification")
vfRFwork <- workflow() %>%
  add_recipe(modRecipe) %>% 
  add_model(vfRFmodel)
vfRFworkGrid <- vfRFwork %>%
  tune_grid(resamples = vfRF, grid = 3)

vfResults <- collect_metrics(vfRFworkGrid)
vfBest <- show_best(vfRFworkGrid, metric = "accuracy") 
vfBest
vfFinal <- finalize_workflow(vfRFwork, select_best(vfRFworkGrid)) %>%
  fit(Train2)
vfFinalPred <- predict(vfFinal, new_data = Train2)
vfFinalAcc <- accuracy(Train2, truth = Product, estimate = vfFinalPred$.pred_class) 
vfFinalAcc
vfFinal
```  

## Modeling test set data

Like the training set, the provided test set has no NAs or missing values.
Unlike the training set, the test set has a problem ID variable in place of 
the true product details.  The test data is wrangled in an almost identical 
fashion as the training data.  The final predicted results are listed below.

```{r}
test <- read_csv("data_complaints_test.csv")
names(test) <- make.names(names(test), unique = TRUE)
skim(test) 

test <- test %>%
	select(-Submitted.via, -State, -ZIP.code) %>%
	mutate(across(c(Company), as.factor))
parsedTest <- test %>%
 	select(problem_id, Consumer.complaint.narrative) %>%
	unnest_tokens(word, Consumer.complaint.narrative) %>%
	anti_join(stop_words) %>%
	filter(!str_detect(word, "xx")) %>%
	filter(!str_detect(word, "\\d")) %>%
	filter(!str_detect(word, "\\s")) %>%
	mutate(word = text_tokens(word, stemmer = "en")) %>%
	count(problem_id, word)
testDTM <- parsedTest %>%
	cast_dtm(problem_id, word, n)                 
tempTrain <- tidyTrain %>%
	mutate(problem_id = temp_id) %>%
	select(-Product, -temp_id, -State, -ZIP.code)
tidyTest <- testDTM %>%
	tidy() %>%
	rename(problem_id = document) %>%
	pivot_wider(names_from = term, 
			values_from = count, 
			names_repair = "minimal", 
			values_fill = 0) %>%
	mutate(problem_id = as.numeric(problem_id)) %>%
	arrange(problem_id) %>%
	left_join(test, by = "problem_id") %>%
	select(-Consumer.complaint.narrative) %>%
  mutate(navient = 0) %>%
	select(names(tempTrain))

vfFinalTestPred <- predict(vfFinal, new_data = tidyTest)
vfFinalTestPred
```

## Conclusions

As demonstrated by the differences between training set accuracy and estimated
error, the Random Forest algorithm clearly "learned" the training set.  The
removal of state and ZIP code predictors marginally improved computation efficiency
at a slight cost of accuracy. The introduction of cross-validation and tuning
failed to recover the lost accuracy and produced a final predicted out-of-sample error
 rate of 85.41%.  Because the model exhibited varying accuracies against different 
product types, the ultimate accuracy of the model against test data will, in 
part, be determined by the mix of the four products within the test data.